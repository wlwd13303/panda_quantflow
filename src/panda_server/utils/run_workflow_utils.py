import contextlib
import os
import traceback
import logging
import uuid
from typing import Any
from bson import ObjectId
from fastapi.concurrency import run_in_threadpool
from panda_server.enums.workflow_run_status import WorkflowStatus
from panda_server.models.workflow_model import WorkflowModel
from panda_server.config.database import mongodb
from panda_server.models.workflow_run_model import (
    WorkflowRunModel,
    WorkflowRunUpdateModel,
)
from panda_plugins.base.work_node_registery import ALL_WORK_NODES
from panda_server.utils.db_storage import save_to_gridfs

logger = logging.getLogger(__name__)


def generate_friendly_error_message(error, node, node_input_model, input_data):
    """ç”Ÿæˆå‹å¥½çš„é”™è¯¯ä¿¡æ¯å’Œä¿®å¤å»ºè®®"""
    error_type = type(error).__name__
    error_str = str(error)
    node_type = node.type
    node_name = node.name

    suggestions = []

    # å¤„ç† Pydantic éªŒè¯é”™è¯¯
    if error_type == "ValidationError" and "Field required" in error_str:
        # è§£æç¼ºå°‘çš„å­—æ®µ
        missing_fields = []
        if "df_factor" in error_str:
            missing_fields.append("df_factor")

        for field in missing_fields:
            suggestions.extend(
                [
                    f"âŒ èŠ‚ç‚¹ '{node_name}' ({node_type}) ç¼ºå°‘å¿…éœ€çš„è¾“å…¥å­—æ®µ: '{field}'",
                    f"",
                    f"ğŸ”§ ä¿®å¤å»ºè®®:",
                    f"1. æ£€æŸ¥å·¥ä½œæµå›¾ä¸­æ˜¯å¦æœ‰èŠ‚ç‚¹è¿æ¥åˆ°è¯¥èŠ‚ç‚¹çš„ '{field}' è¾“å…¥ç«¯å£",
                    f"2. å¸¸è§çš„ {field} æ•°æ®æºèŠ‚ç‚¹:",
                ]
            )

            if field == "df_factor":
                suggestions.extend(
                    [
                        f"   - å…¬å¼èŠ‚ç‚¹ (formula_node): è¾“å‡ºå­—æ®µ 'df' æˆ– 'result'",
                        f"   - å› å­æ„å»ºèŠ‚ç‚¹ (factor_build_node): è¾“å‡ºå­—æ®µ 'factor'",
                        f"   - æ–¯çš®å°”æ›¼å› å­æ„å»ºèŠ‚ç‚¹ (spearman_factor_build_node): è¾“å‡ºå­—æ®µ 'factor'",
                        f"   - PCAå› å­æ„å»ºèŠ‚ç‚¹ (pca_factor_build_node): è¾“å‡ºå­—æ®µ 'factor'",
                    ]
                )
            elif field == "train_data":
                suggestions.extend(
                    [
                        f"   - CSVè¯»å–èŠ‚ç‚¹ (read_csv_node): è¾“å‡ºå­—æ®µ 'df'",
                        f"   - ç‰¹å¾å·¥ç¨‹èŠ‚ç‚¹ (feature_engineering_node): è¾“å‡ºå­—æ®µ 'processed_data'",
                        f"   - å› å­æ„å»ºèŠ‚ç‚¹ (factor_build_node): è¾“å‡ºå­—æ®µ 'factor'",
                    ]
                )

            suggestions.extend(
                [
                    f"",
                    f"3. è¿æ¥é…ç½®æ­¥éª¤:",
                    f"   - æ‰¾åˆ°è¾“å‡º {field} ç›¸å…³æ•°æ®çš„èŠ‚ç‚¹",
                    f"   - å°†è¯¥èŠ‚ç‚¹çš„è¾“å‡ºç«¯å£è¿æ¥åˆ°å½“å‰èŠ‚ç‚¹çš„ '{field}' è¾“å…¥ç«¯å£",
                    f"   - ç¡®ä¿å­—æ®µæ˜ å°„æ­£ç¡®",
                    f"",
                    f"4. å½“å‰æ¥æ”¶åˆ°çš„è¾“å…¥å­—æ®µ: {list(input_data.keys())}",
                    f"   ç¼ºå°‘çš„å¿…éœ€å­—æ®µ: {field}",
                ]
            )

    # å¤„ç†å…¶ä»–å¸¸è§é”™è¯¯
    elif "DataFrame" in error_str:
        suggestions.extend(
            [
                f"âŒ æ•°æ®æ ¼å¼é”™è¯¯: æœŸæœ› pandas DataFrameï¼Œä½†æ¥æ”¶åˆ°å…¶ä»–ç±»å‹",
                f"",
                f"ğŸ”§ ä¿®å¤å»ºè®®:",
                f"1. æ£€æŸ¥å‰ç½®èŠ‚ç‚¹æ˜¯å¦æ­£ç¡®è¾“å‡º DataFrame æ ¼å¼çš„æ•°æ®",
                f"2. éªŒè¯è¿æ¥çš„å­—æ®µæ˜¯å¦åŒ…å«æœ‰æ•ˆçš„ DataFrame",
                f"3. æ£€æŸ¥å‰ç½®èŠ‚ç‚¹çš„æ‰§è¡Œæ—¥å¿—ï¼Œç¡®è®¤æ•°æ®ç”Ÿæˆæ­£å¸¸",
            ]
        )

    elif "import" in error_str.lower() or "module" in error_str.lower():
        suggestions.extend(
            [
                f"âŒ æ¨¡å—å¯¼å…¥é”™è¯¯",
                f"",
                f"ğŸ”§ ä¿®å¤å»ºè®®:",
                f"1. æ£€æŸ¥ç›¸å…³ä¾èµ–åŒ…æ˜¯å¦å·²å®‰è£…",
                f"2. éªŒè¯ Python è·¯å¾„é…ç½®",
                f"3. é‡å¯æœåŠ¡ä»¥é‡æ–°åŠ è½½æ¨¡å—",
            ]
        )

    else:
        suggestions.extend(
            [
                f"âŒ èŠ‚ç‚¹æ‰§è¡Œé”™è¯¯: {error_str}",
                f"",
                f"ğŸ”§ é€šç”¨ä¿®å¤å»ºè®®:",
                f"1. æ£€æŸ¥èŠ‚ç‚¹é…ç½®å‚æ•°æ˜¯å¦æ­£ç¡®",
                f"2. éªŒè¯è¾“å…¥æ•°æ®æ ¼å¼å’Œå†…å®¹",
                f"3. æŸ¥çœ‹èŠ‚ç‚¹æ‰§è¡Œæ—¥å¿—è·å–æ›´å¤šä¿¡æ¯",
                f"4. å°è¯•å•ç‹¬æµ‹è¯•è¯¥èŠ‚ç‚¹åŠŸèƒ½",
            ]
        )

    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    suggestions.extend(
        [
            f"",
            f"ğŸ“Š è°ƒè¯•ä¿¡æ¯:",
            f"- èŠ‚ç‚¹ç±»å‹: {node_type}",
            f"- èŠ‚ç‚¹åç§°: {node_name}",
            f"- é”™è¯¯ç±»å‹: {error_type}",
            f"- æ¥æ”¶åˆ°çš„è¾“å…¥å­—æ®µ: {list(input_data.keys())}",
        ]
    )

    # å°è¯•è·å–æ¨¡å‹å­—æ®µä¿¡æ¯
    try:
        if hasattr(node_input_model, "model_fields"):
            required_fields = []
            optional_fields = []

            for field_name, field_info in node_input_model.model_fields.items():
                if field_info.is_required():
                    required_fields.append(field_name)
                else:
                    default_value = getattr(field_info, "default", None)
                    optional_fields.append(f"{field_name} (é»˜è®¤: {default_value})")

            suggestions.extend(
                [
                    f"- å¿…éœ€å­—æ®µ: {required_fields}",
                    f"- å¯é€‰å­—æ®µ: {optional_fields}",
                ]
            )
    except:
        pass

    return "\n".join(suggestions)


async def run_workflow_in_background(workflow_run_id):
    # ç”Ÿæˆå”¯ä¸€æ‰§è¡ŒID
    execution_id = str(uuid.uuid4())[:8]

    logger.info(
        f"ğŸš€ [EXEC:{execution_id}] run_workflow_logic: start, workflow_run_id: {workflow_run_id}"
    )

    # ä» mongodb ä¸­è·å– workflow run ä¿¡æ¯
    workflow_run_collection = mongodb.get_collection("workflow_run")
    query_result = await workflow_run_collection.find_one(
        {"_id": ObjectId(workflow_run_id)}
    )
    if not query_result:
        logger.error(f"No workflow run found, id: {workflow_run_id}")
        return

    workflow_run = WorkflowRunModel(**query_result)

    workflow_id = workflow_run.workflow_id

    # åˆ›å»ºå·¥ä½œæµçº§åˆ«çš„ç”¨æˆ·æ—¥å¿—è®°å½•å™¨
    try:
        from common.logging.workflow_log import WorkflowLogger
        workflow_logger = WorkflowLogger(
            user_id=workflow_run.owner,
            workflow_run_id=workflow_run_id,
            work_node_id=None,  # Noneè¡¨ç¤ºå·¥ä½œæµçº§åˆ«
        )
        await workflow_logger.info("å·¥ä½œæµå¼€å§‹æ‰§è¡Œ", workflow_id=workflow_id)
    except Exception as e:
        logger.error(f"Failed to create workflow logger, terminating workflow: {e}")
        return

    # ä» mongodb ä¸­è·å– workflow ä¿¡æ¯
    workflow_collection = mongodb.get_collection("workflow")
    query_result = await workflow_collection.find_one({"_id": ObjectId(workflow_id)})
    if not query_result:
        logger.error(f"No workflow found, id: {workflow_id}")
        if workflow_logger:
            await workflow_logger.error("æœªæ‰¾åˆ°å·¥ä½œæµå®šä¹‰", workflow_id=workflow_id)
        return

    workflow = WorkflowModel(**query_result)

    # å¾—åˆ°åˆ†å±‚æ’åºå¥½çš„èŠ‚ç‚¹åˆ—è¡¨
    try:
        execution_layers = determine_workflow_execution_order(workflow)
        await workflow_logger.info(
            "å·¥ä½œæµæ‰§è¡Œé¡ºåºç¡®å®šå®Œæˆ",
            workflow_id=workflow_id,
            layers_count=len(execution_layers),
            total_nodes=sum(len(layer) for layer in execution_layers),
        )
    except Exception as e:
        logger.error(
            f"Error determining workflow execution order, id: {workflow_run_id}, error: {e}"
        )
        await workflow_logger.error(
            "å·¥ä½œæµæ‰§è¡Œé¡ºåºç¡®å®šå¤±è´¥", workflow_id=workflow_id, error=str(e)
        )
        await mark_workflow_run_failed(workflow_run_id, str(e), traceback.format_exc())
        workflow_run_update_data = WorkflowRunUpdateModel(status=WorkflowStatus.FAILED)
        workflow_run_collection = mongodb.get_collection("workflow_run")
        await workflow_run_collection.update_one(
            {"_id": ObjectId(workflow_run_id)},
            {"$set": workflow_run_update_data.model_dump(exclude_unset=True)},
        )
        return

    logger.info(
        f"ğŸ”„ [EXEC:{execution_id}] run_workflow_logic: execution_order determined: {execution_layers}"
    )

    # ç®€åŒ–ç‰ˆçš„æ‰§è¡Œé€»è¾‘ (å•çº¿ç¨‹æ‰§è¡Œ)
    node_outputs: dict[str, Any] = {}
    failed_node_ids = []
    success_node_ids = []
    passed_link_ids = []
    for layer_index, layer in enumerate(execution_layers):
        if await is_workflow_run_terminated(workflow_run_id):
            logger.info(f"Workflow run terminated, id: {workflow_run_id}")
            await workflow_logger.warning("å·¥ä½œæµæ‰§è¡Œè¢«æ‰‹åŠ¨ç»ˆæ­¢", workflow_id=workflow_id)
            return
        logger.info(
            f"âš¡ [EXEC:{execution_id}] run_workflow_logic: running layer: {layer}"
        )
        await workflow_logger.info(
            f"å¼€å§‹æ‰§è¡Œç¬¬ {layer_index + 1} å±‚èŠ‚ç‚¹",
            workflow_id=workflow_id,
            layer_index=layer_index + 1,
            nodes_in_layer=len(layer),
        )
        progress = layer_index / len(execution_layers) * 100
        workflow_run_update_data = WorkflowRunUpdateModel(
            status=WorkflowStatus.RUNNING,
            progress=progress,
            running_node_ids=layer,
            failed_node_ids=failed_node_ids,
            success_node_ids=success_node_ids,
            passed_link_ids=passed_link_ids,
        )
        workflow_run_collection = mongodb.get_collection("workflow_run")
        await workflow_run_collection.update_one(
            {"_id": ObjectId(workflow_run_id)},
            {"$set": workflow_run_update_data.model_dump(exclude_unset=True)},
        )
        logger.debug(
            f"run_workflow_logic: workflow_run_update_data: {workflow_run_update_data}"
        )

        # å¹¶è¡Œæ‰§è¡Œå½“å‰å±‚çš„æ‰€æœ‰èŠ‚ç‚¹
        for node_id in layer:
            if await is_workflow_run_terminated(workflow_run_id):
                logger.info(f"Workflow run terminated, id: {workflow_run_id}")
                await workflow_logger.warning(
                    "å·¥ä½œæµæ‰§è¡Œè¢«æ‰‹åŠ¨ç»ˆæ­¢", workflow_id=workflow_id, work_node_id=node_id
                )
                return
            try:
                # è·å–èŠ‚ç‚¹ä¿¡æ¯
                node = [n for n in workflow.nodes if n.uuid == node_id][0]
                logger.info(
                    f"ğŸ”§ [EXEC:{execution_id}] run_workflow_logic: running work node id: {node_id}, name: {node.name}"
                )
                node_class = ALL_WORK_NODES.get(node.name)
                node_instance = node_class()
                # è®¾ç½®èŠ‚ç‚¹çš„æ—¥å¿—ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨æˆ·åœ¨èŠ‚ç‚¹ä¸­è°ƒç”¨ self.log_info ç­‰æ–¹æ³•æ—¶èƒ½å­˜å‚¨åˆ°æ•°æ®åº“
                node_instance._setup_logging_context(
                    user_id=workflow_run.owner,
                    workflow_run_id=workflow_run_id,
                    work_node_id=node_id,
                    workflow_id=workflow_id
                )
                node_input_model = node_class.input_model()
                # æ³¨å…¥é™æ€çš„è¾“å…¥å­—æ®µ
                input_data = node.static_input_data.copy()
                logger.debug(
                    f"ğŸ“Š [EXEC:{execution_id}] run_workflow_logic: got static input data: {input_data}"
                )
                # æ³¨å…¥åŠ¨æ€çš„ä»å‰ç½®èŠ‚ç‚¹è·å¾—çš„è¾“å…¥å­—æ®µ
                previous_links = [
                    link for link in workflow.links if link.next_node_uuid == node.uuid
                ]
                for link in previous_links:
                    previous_node_uuid = link.previous_node_uuid
                    target_input_data = (
                        node_outputs[previous_node_uuid]
                        .model_dump()
                        .get(link.input_field_name)
                    )
                    logger.debug(
                        f"ğŸ”— [EXEC:{execution_id}] run_workflow_logic: got data from link: link_input_field_name: {link.input_field_name}, link_output_field_name: {link.output_field_name}, data: {target_input_data}"
                    )
                    input_data[link.output_field_name] = target_input_data
                logger.info(
                    f"â–¶ï¸ [EXEC:{execution_id}] run_workflow_logic: running work node id: {node_id}, name: {node.name}, got input_data: {input_data}"
                )
                node_input = node_input_model(**input_data)
                await workflow_logger.debug(
                    "èŠ‚ç‚¹è¾“å…¥æ•°æ®", workflow_id=workflow_id, work_node_id=node_id, input_fields=list(input_data.keys())
                )
                node_output = await run_in_threadpool(
                    lambda: run_without_stdout(node_instance.run, node_input)
                )
                # å¤„ç†èŠ‚ç‚¹æ‰§è¡ŒæœŸé—´äº§ç”Ÿçš„é˜Ÿåˆ—æ—¥å¿—
                await node_instance._process_queued_logs()
                await workflow_logger.info(
                    f"èŠ‚ç‚¹ {node.name} æ‰§è¡ŒæˆåŠŸ", workflow_id=workflow_id, work_node_id=node_id, has_output=node_output is not None
                )
                node_outputs[node_id] = node_output
                # ä¿å­˜èŠ‚ç‚¹è¾“å‡ºåˆ°æ•°æ®åº“
                output_db_id = await save_output_to_db(
                    workflow_run_id, node_id, workflow_run.owner, node_output
                )
                node.output_db_id = output_db_id
                # æ›´æ–°æˆåŠŸèŠ‚ç‚¹
                success_node_ids.append(node_id)
                # æ›´æ–°é€šè¿‡çš„è¿æ¥
                passed_link_ids.extend(link.uuid for link in previous_links)
            except Exception as e:
                failed_node_ids.append(node_id)
                stack_trace = traceback.format_exc()

                # å¤„ç†èŠ‚ç‚¹æ‰§è¡ŒæœŸé—´äº§ç”Ÿçš„é˜Ÿåˆ—æ—¥å¿—ï¼ˆå³ä½¿èŠ‚ç‚¹å¤±è´¥ï¼‰
                try:
                    await node_instance._process_queued_logs()
                except Exception as log_error:
                    logger.warning(f"Failed to process queued logs for failed node {node_id}: {log_error}")

                # ç”Ÿæˆå‹å¥½çš„é”™è¯¯ä¿¡æ¯
                friendly_error = generate_friendly_error_message(
                    e, node, node_input_model, input_data
                )
                logger.error(
                    f"Error running workflow, id: {workflow_run_id}, failed node: {node_id} error: {e},\nstack_trace: {stack_trace}\n\n=== é”™è¯¯åˆ†æä¸ä¿®å¤å»ºè®® ===\n{friendly_error}"
                )
                await mark_workflow_run_failed(
                    workflow_run_id, str(e), stack_trace, failed_node_ids
                )
                await workflow_logger.error(
                    "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥",
                    workflow_id=workflow_id,
                    work_node_id=node_id,
                    node_name=node.name,
                    error=str(e),
                    suggestions=friendly_error,
                )
                workflow_run_update_data = WorkflowRunUpdateModel(
                    status=WorkflowStatus.FAILED,
                    failed_node_ids=failed_node_ids,
                    last_error_message=str(e),
                    last_error_stacktrace=stack_trace,
                )
                workflow_run_collection = mongodb.get_collection("workflow_run")
                await workflow_run_collection.update_one(
                    {"_id": ObjectId(workflow_run_id)},
                    {"$set": workflow_run_update_data.model_dump(exclude_unset=True)},
                )
                return

    logger.info(
        f"run_workflow_logic: all nodes executed successfully, workflow_run_id: {workflow_run_id}"
    )
    await workflow_logger.info(
        "å·¥ä½œæµæ‰§è¡Œå®Œæˆ", workflow_id=workflow_id, total_nodes=len(success_node_ids)
    )
    workflow_run_update_data = WorkflowRunUpdateModel(
        status=WorkflowStatus.SUCCESS,
        progress=100,
        running_node_ids=[],
        failed_node_ids=[],
        success_node_ids=[node.uuid for node in workflow.nodes],
        passed_link_ids=[link.uuid for link in workflow.links],
        output_data_obj={node.uuid: node.output_db_id for node in workflow.nodes},
    )
    workflow_run_collection = mongodb.get_collection("workflow_run")
    await workflow_run_collection.update_one(
        {"_id": ObjectId(workflow_run_id)},
        {"$set": workflow_run_update_data.model_dump(exclude_unset=True)},
    )


async def is_workflow_run_terminated(workflow_run_id):
    workflow_run_collection = mongodb.get_collection("workflow_run")
    workflow_run_query_result = await workflow_run_collection.find_one(
        {"_id": ObjectId(workflow_run_id)}
    )
    if workflow_run_query_result.get("status") == WorkflowStatus.MANUAL_STOP:
        return True
    return False


async def save_output_to_db(workflow_run_id, node_id, owner, node_output) -> str:
    extra = {
        "workflow_run_id": workflow_run_id,
        "node_id": node_id,
        "owner": owner,
    }
    return await save_to_gridfs("workflow_node_output_fs", node_output, extra=extra)


def determine_workflow_execution_order(workflow_model):
    """
    ç¡®å®šå·¥ä½œæµèŠ‚ç‚¹çš„æ‰§è¡Œé¡ºåº

    å‚æ•°:
        workflow_model: WorkflowModel å¯¹è±¡

    è¿”å›:
        æ‰§è¡Œåºåˆ—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„èŠ‚ç‚¹åˆ—è¡¨
        äºŒç»´åˆ—è¡¨ä¾‹å¦‚: [[node_id1, node_id2], [node_id3], [node_id4, node_id5]]
    """
    # 1. æ„å»ºèŠ‚ç‚¹ä¾èµ–å…³ç³»å›¾
    node_dependencies = {}  # è®°å½•æ¯ä¸ªèŠ‚ç‚¹ä¾èµ–çš„å‰ç½®èŠ‚ç‚¹æ•°é‡
    node_successors = {}  # è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„åç»­èŠ‚ç‚¹

    # åˆå§‹åŒ–ä¾èµ–å…³ç³»
    for node in workflow_model.nodes:
        node_dependencies[node.uuid] = 0
        node_successors[node.uuid] = []

    # å¡«å……ä¾èµ–å…³ç³»
    for link in workflow_model.links:
        from_node = link.previous_node_uuid
        to_node = link.next_node_uuid

        # å¢åŠ ç›®æ ‡èŠ‚ç‚¹çš„ä¾èµ–è®¡æ•°
        node_dependencies[to_node] += 1

        # æ·»åŠ æºèŠ‚ç‚¹çš„åç»§èŠ‚ç‚¹
        node_successors[from_node].append(to_node)

    # 2. æ‰¾å‡ºæ‰€æœ‰å…¥åº¦ä¸º0çš„èŠ‚ç‚¹ï¼ˆæ²¡æœ‰ä¾èµ–çš„èŠ‚ç‚¹ï¼‰ä½œä¸ºèµ·å§‹ç‚¹
    start_nodes = [
        node_id
        for node_id, dependencies in node_dependencies.items()
        if dependencies == 0
    ]

    if not start_nodes:
        # å¦‚æœæ²¡æœ‰èµ·å§‹èŠ‚ç‚¹ï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯ä¾èµ–
        raise ValueError("å·¥ä½œæµä¸­å­˜åœ¨å¾ªç¯ä¾èµ–ï¼Œæ— æ³•ç¡®å®šæ‰§è¡Œé¡ºåº")

    # 3. æ‰§è¡Œæ‹“æ‰‘æ’åºï¼ŒæŒ‰å±‚æ¬¡ç»„ç»‡èŠ‚ç‚¹
    execution_layers = []
    remaining_nodes = set(node_dependencies.keys())

    while start_nodes:
        # å½“å‰å±‚å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„èŠ‚ç‚¹
        current_layer = start_nodes
        execution_layers.append(current_layer)

        # ç§»é™¤å·²å¤„ç†çš„èŠ‚ç‚¹
        remaining_nodes -= set(current_layer)

        # æŸ¥æ‰¾ä¸‹ä¸€å±‚èŠ‚ç‚¹
        next_layer = []
        for node_id in current_layer:
            # å¤„ç†å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰åç»§èŠ‚ç‚¹
            for successor in node_successors[node_id]:
                # å‡å°‘ä¾èµ–è®¡æ•°
                node_dependencies[successor] -= 1

                # å¦‚æœæ‰€æœ‰ä¾èµ–éƒ½å·²æ»¡è¶³ï¼ŒåŠ å…¥ä¸‹ä¸€å±‚
                if node_dependencies[successor] == 0:
                    next_layer.append(successor)

        # æ›´æ–°èµ·å§‹èŠ‚ç‚¹ä¸ºä¸‹ä¸€å±‚èŠ‚ç‚¹
        start_nodes = next_layer

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹éƒ½å·²å¤„ç†
    if remaining_nodes:
        # å¦‚æœè¿˜æœ‰æœªå¤„ç†çš„èŠ‚ç‚¹ï¼Œè¯´æ˜å­˜åœ¨ç¯å½¢ä¾èµ–
        raise ValueError(f"å·¥ä½œæµä¸­å­˜åœ¨æ— æ³•åˆ°è¾¾çš„èŠ‚ç‚¹æˆ–å¾ªç¯ä¾èµ–: {remaining_nodes}")

    return execution_layers


async def mark_workflow_run_failed(
    workflow_run_id, error_message, error_stacktrace, failed_node_ids=[]
):
    """
    æ ‡è®°å·¥ä½œæµè¿è¡Œå¤±è´¥
    """
    workflow_run_update_data = WorkflowRunUpdateModel(
        status=WorkflowStatus.FAILED,
        last_error_message=error_message,
        last_error_stacktrace=error_stacktrace,
        failed_node_ids=failed_node_ids,
    )
    workflow_run_collection = mongodb.get_collection("workflow_run")
    await workflow_run_collection.update_one(
        {"_id": ObjectId(workflow_run_id)},
        {"$set": workflow_run_update_data.model_dump(exclude_unset=True)},
    )


# ç¦ç”¨æ§åˆ¶å°è¾“å‡ºè¿è¡ŒæŸæ®µé€»è¾‘
def run_without_stdout(func, *args, **kwargs):
    # with open(os.devnull, "w") as devnull:
    #     with contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):
    return func(*args, **kwargs)


# TODO å¾…å®Œå–„, å®Œæ•´çš„å¤šçº¿ç¨‹å·¥ä½œæµæ‰§è¡Œ+æ¯ä¸€æ­¥å­˜å‚¨çŠ¶æ€çš„æ–¹æ³•
def execute_workflow(workflow_model, workflow_run_id):
    """
    æ‰§è¡Œå·¥ä½œæµï¼ŒæŒ‰ç…§ç¡®å®šçš„é¡ºåºä¾æ¬¡æ‰§è¡ŒèŠ‚ç‚¹

    å‚æ•°:
        workflow_model: WorkflowModel å¯¹è±¡
        workflow_run_id: å·¥ä½œæµè¿è¡ŒID
    """
    # ç¡®å®šæ‰§è¡Œé¡ºåº
    execution_layers = determine_workflow_execution_order(workflow_model)

    # è·Ÿè¸ªå®Œæˆçš„èŠ‚ç‚¹å’Œå…¶è¾“å‡º
    completed_nodes = {}

    # æ›´æ–°å·¥ä½œæµè¿è¡ŒçŠ¶æ€ä¸ºè¿è¡Œä¸­
    update_workflow_run_status(workflow_run_id, WorkflowStatus.RUNNING)

    # æŒ‰å±‚æ¬¡æ‰§è¡ŒèŠ‚ç‚¹
    for layer_index, layer in enumerate(execution_layers):
        # å½“å‰å±‚èŠ‚ç‚¹è¿›åº¦å æ¯”
        layer_progress = 100.0 / len(execution_layers)

        # æ›´æ–°è¿è¡Œä¸­çš„èŠ‚ç‚¹
        update_running_nodes(workflow_run_id, layer)

        # å¹¶è¡Œæ‰§è¡Œå½“å‰å±‚çš„æ‰€æœ‰èŠ‚ç‚¹
        layer_results = {}

        for node_id in layer:
            try:
                # è·å–èŠ‚ç‚¹ä¿¡æ¯
                node = get_node_by_id(workflow_model, node_id)

                # æ”¶é›†èŠ‚ç‚¹è¾“å…¥æ•°æ®ï¼ˆä»å‰ç½®èŠ‚ç‚¹å’Œé™æ€è¾“å…¥ï¼‰
                inputs = collect_node_inputs(
                    node, completed_nodes, workflow_model.links
                )

                # æ‰§è¡ŒèŠ‚ç‚¹
                result = execute_node(node, inputs)

                # è®°å½•æˆåŠŸèŠ‚ç‚¹å’Œç»“æœ
                layer_results[node_id] = result
                add_success_node(workflow_run_id, node_id)

                # å­˜å‚¨èŠ‚ç‚¹è¾“å‡ºåˆ°æ•°æ®åº“
                output_db_id = store_node_output(result)
                update_node_output_id(workflow_run_id, node_id, output_db_id)

            except Exception as e:
                # è®°å½•å¤±è´¥èŠ‚ç‚¹
                add_failed_node(workflow_run_id, node_id)
                log_node_error(workflow_run_id, node_id, str(e))

        # æ›´æ–°å·²å®ŒæˆèŠ‚ç‚¹
        completed_nodes.update(layer_results)

        # æ›´æ–°å·¥ä½œæµè¿›åº¦
        current_progress = (layer_index + 1) * layer_progress
        update_workflow_progress(workflow_run_id, current_progress)

        # æ›´æ–°å·²é€šè¿‡çš„è¿æ¥
        update_passed_links(workflow_run_id, layer, workflow_model.links)

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹éƒ½æˆåŠŸå®Œæˆ
    all_nodes = set(node.uuid for node in workflow_model.nodes)
    failed_nodes = get_failed_nodes(workflow_run_id)

    if failed_nodes:
        # å­˜åœ¨å¤±è´¥èŠ‚ç‚¹ï¼Œå·¥ä½œæµå¤±è´¥
        update_workflow_run_status(workflow_run_id, WorkflowStatus.FAILED)
    else:
        # æ‰€æœ‰èŠ‚ç‚¹éƒ½æˆåŠŸå®Œæˆï¼Œå·¥ä½œæµæˆåŠŸ
        update_workflow_run_status(workflow_run_id, WorkflowStatus.SUCCESS)
